{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "expressionDic = {\n",
    "\"0\": \"angry\",\n",
    "\"1\" : \"disgust\",\n",
    "\"2\" : \"fear\",\n",
    "\"3\" : \"happy\",\n",
    "\"4\" : \"sad\",\n",
    "\"5\" : \"surprise\",\n",
    "\"6\" : \"neutral\"}\n",
    "\n",
    "#incomplete\n",
    "#https://github.com/tensorflow/tfjs-models/blob/838611c02f51159afdd77469ce67f0e26b7bbb23/face-landmarks-detection/src/mediapipe-facemesh/keypoints.ts\n",
    "#https://github.com/tensorflow/tfjs-models/blob/646992fd7ab8237c0dc908f2526301414b417c95/face-landmarks-detection/mesh_map.jpg\n",
    "facialLandmarks = {\n",
    "  'silhouette': [\n",
    "    10,  338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288,\n",
    "    397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136,\n",
    "    172, 58,  132, 93,  234, 127, 162, 21,  54,  103, 67,  109\n",
    "  ],\n",
    "\n",
    "  'lipsUpperOuter': [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, 291],\n",
    "  'lipsLowerOuter': [146, 91, 181, 84, 17, 314, 405, 321, 375, 291],\n",
    "  'lipsUpperInner': [78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308],\n",
    "  'lipsLowerInner': [78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308],\n",
    "\n",
    "  'rightEyeUpper0': [246, 161, 160, 159, 158, 157, 173],\n",
    "  'rightEyeLower0': [33, 7, 163, 144, 145, 153, 154, 155, 133],\n",
    "  'rightEyeUpper1': [247, 30, 29, 27, 28, 56, 190],\n",
    "  'rightEyeLower1': [130, 25, 110, 24, 23, 22, 26, 112, 243],\n",
    "  'rightEyeUpper2': [113, 225, 224, 223, 222, 221, 189],\n",
    "  'rightEyeLower2': [226, 31, 228, 229, 230, 231, 232, 233, 244],\n",
    "  'rightEyeLower3': [143, 111, 117, 118, 119, 120, 121, 128, 245],\n",
    "\n",
    "  'rightEyebrowUpper': [156, 70, 63, 105, 66, 107, 55, 193],\n",
    "  'rightEyebrowLower': [35, 124, 46, 53, 52, 65],\n",
    "\n",
    "  'rightEyeIris': [473, 474, 475, 476, 477],\n",
    "\n",
    "  'leftEyeUpper0': [466, 388, 387, 386, 385, 384, 398],\n",
    "  'leftEyeLower0': [263, 249, 390, 373, 374, 380, 381, 382, 362],\n",
    "  'leftEyeUpper1': [467, 260, 259, 257, 258, 286, 414],\n",
    "  'leftEyeLower1': [359, 255, 339, 254, 253, 252, 256, 341, 463],\n",
    "  'leftEyeUpper2': [342, 445, 444, 443, 442, 441, 413],\n",
    "  'leftEyeLower2': [446, 261, 448, 449, 450, 451, 452, 453, 464],\n",
    "  'leftEyeLower3': [372, 340, 346, 347, 348, 349, 350, 357, 465],\n",
    "\n",
    "  'leftEyebrowUpper': [383, 300, 293, 334, 296, 336, 285, 417],\n",
    "  'leftEyebrowLower': [265, 353, 276, 283, 282, 295],\n",
    "\n",
    "  'leftEyeIris': [468, 469, 470, 471, 472],\n",
    "\n",
    "  'midwayBetweenEyes': [168],\n",
    "\n",
    "  'noseTip': [1],\n",
    "  'noseBottom': [2],\n",
    "  'noseRightCorner': [98],\n",
    "  'noseLeftCorner': [327],\n",
    "\n",
    "  'rightCheek': [205],\n",
    "  'leftCheek': [425]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def alignNormalizedPointsFixed(xIn : list, yIn: list):\n",
    "    baseNoseX = xIn[0]\n",
    "    baseNoseY = yIn[0]\n",
    "\n",
    "    targetX = .5\n",
    "    targetY = .5\n",
    "\n",
    "    deltaX = targetX - baseNoseX\n",
    "    deltaY = targetY - baseNoseY\n",
    "\n",
    "    xShift = []\n",
    "    yShift = []\n",
    "\n",
    "    for x, y in zip(xIn, yIn):\n",
    "        xShift.append(x + deltaX)\n",
    "        yShift.append(y + deltaY) \n",
    "\n",
    "    return xShift, yShift\n",
    "\n",
    "\n",
    "def scalePointsFixed(xIn: list, yIn: list, zIn: list):\n",
    "\n",
    "    #controlDistBetweenEyes = xCon[243] - xCon[362]\n",
    "    #hard coding control distance to scale all image to coming from 'same face'\n",
    "    controlDistBetweenEyes = .05\n",
    "    testDistBetweenEyes = xIn[243] - xIn[362]\n",
    "\n",
    "    scale = controlDistBetweenEyes / testDistBetweenEyes\n",
    "\n",
    "    xOut = list(map(lambda x: x * scale, xIn))\n",
    "    yOut = list(map(lambda y: y * scale, yIn))\n",
    "    zOut = list(map(lambda z: z * scale, zIn))\n",
    "    \n",
    "    return xOut, yOut, zOut\n",
    "\n",
    "def flipPoints(input: list):\n",
    "    flippedPoints = [-p for p in input]\n",
    "    return flippedPoints\n",
    "\n",
    "def reversePoints(input: list):\n",
    "    reveredPoints = [1-p for p in input]\n",
    "    return reveredPoints\n",
    "\n",
    "def rotate90CC(affine: np.array):\n",
    "    rotateMatrix = np.array([[0, 1],\n",
    "                             [-1, 0]])\n",
    "    return np.dot(affine, rotateMatrix)\n",
    "\n",
    "def affineTransform(X: list, Y: list):\n",
    "    #https://stackoverflow.com/questions/74493141/align-x-and-y-coordinates-of-face-landmarks-in-r\n",
    "    #133 is left eye tearduct, 362 is right\n",
    "    #deltaX = X[133] - X[362]\n",
    "    #deltaY = Y[133] - Y[362]\n",
    "\n",
    "    #10 is top of forehead, 152 is bottom of chin\n",
    "    deltaX = X[10] - X[152]\n",
    "    deltaY = Y[10] - Y[152]\n",
    "    theta = math.atan2(-deltaY, deltaX)\n",
    "\n",
    "    rotationMatrix = np.array([[math.cos(theta), -math.sin(theta)],\n",
    "                              [math.sin(theta), math.cos(theta)]])\n",
    "    \n",
    "    rotX = []\n",
    "    rotY = []\n",
    "\n",
    "    for x, y in zip(X, Y):\n",
    "        rotateCoord = np.dot(rotationMatrix, np.array([x, y]))\n",
    "        rotX.append(rotateCoord[0]) \n",
    "        rotY.append(rotateCoord[1])\n",
    "    \n",
    "    rotMatrix = np.column_stack((rotX, rotY))\n",
    "\n",
    "    rotPoints = rotate90CC(rotMatrix)\n",
    "    rotX = rotPoints[:, 0].tolist()\n",
    "    rotY = rotPoints[:,1].tolist()\n",
    "\n",
    "    return rotX, rotY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "imageDir = r\"C:\\Users\\myfir\\My Drive\\Skyline\\ExpW\\data\\data\\image\\originPad\\origin\"\n",
    "labelListPath = r\"C:\\Users\\myfir\\My Drive\\Skyline\\ExpW\\data\\data\\label\\label.lst\"\n",
    "headers = [\"image_name\", \"face_id_in_image\", \"face_box_top\" ,\"face_box_left\" ,\"face_box_right\" ,\"face_box_bottom\" ,\"face_box_confidence\" , \"expression_label\"]\n",
    "\n",
    "data = pd.read_csv(labelListPath, delim_whitespace=True, header= None, names= headers, dtype= str)\n",
    "data['expression_label'] = data['expression_label'].replace(expressionDic)\n",
    "data= data.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "data = data[['image_name', 'expression_label']]\n",
    "data = data.drop_duplicates(subset=['image_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "\n",
    "# STEP 2: Create an FaceLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# Connect to the SQLite database (or create it if it doesn't exist)\n",
    "dbPath = r'C:\\Users\\myfir\\My Drive\\Skyline\\ExpW\\expw.db'\n",
    "conn = sqlite3.connect(dbPath)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table with the desired schema\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS landmarksInput (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    NAME TEXT,\n",
    "    EXPR TEXT,\n",
    "    X TEXT,\n",
    "    Y TEXT,\n",
    "    Z TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS landmarksTransform (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    NAME TEXT,\n",
    "    EXPR TEXT,\n",
    "    X TEXT,\n",
    "    Y TEXT,\n",
    "    Z TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# Commit the changes\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dirNames = list(name for name in os.listdir(imageDir) if name.endswith('.jpg'))\n",
    "imageNames = list(data['image_name'])\n",
    "bad = []\n",
    "\n",
    "for name in dirNames:\n",
    "    if name not in imageNames:\n",
    "        bad.append(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dirNames.sort()\n",
    "print(repr(bad[0]) == repr(dirNames[5])) #'afraid_american_513.jpg'\n",
    "print(repr(bad[0])) #'afraid_american_513.jpg'\n",
    "print(repr(dirNames[5])) #'afraid_american_513.jpg'\n",
    "\n",
    "print(len((set(list(data['image_name'])))))\n",
    "\n",
    "# Convert both lists to lowercase for case-insensitive comparison\n",
    "dirNames_lower = [name.lower() for name in dirNames]\n",
    "imageNames_lower = [name.lower() for name in imageNames]\n",
    "\n",
    "intersection_lower = set(dirNames_lower) & set(imageNames_lower)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenameList = data['image_name'].tolist()\n",
    "#labelList = data['expression_label'].tolist()\n",
    "\n",
    "\n",
    "filenamesList = [name for name in set(os.listdir(imageDir)) if name.endswith('.jpg') and name in set(list(data['image_name']))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\myfir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500 images at 22:42:39\n",
      "Processed 1000 images at 22:42:47\n",
      "Processed 1500 images at 22:42:55\n",
      "Processed 2000 images at 22:43:02\n",
      "Processed 2500 images at 22:43:11\n",
      "Processed 3000 images at 22:43:19\n",
      "Processed 3500 images at 22:43:28\n",
      "Processed 4000 images at 22:43:36\n",
      "Processed 4500 images at 22:43:45\n",
      "Processed 5000 images at 22:43:53\n",
      "Processed 5500 images at 22:44:02\n",
      "Processed 6000 images at 22:44:10\n",
      "Processed 6500 images at 22:44:19\n",
      "Processed 7000 images at 22:44:28\n",
      "Processed 7500 images at 22:44:36\n",
      "Processed 8000 images at 22:44:44\n",
      "Processed 8500 images at 22:44:54\n",
      "Processed 9000 images at 22:45:03\n",
      "Processed 9500 images at 22:45:12\n",
      "Processed 10000 images at 22:45:22\n",
      "Processed 10500 images at 22:45:30\n",
      "Processed 11000 images at 22:45:39\n",
      "Processed 11500 images at 22:45:48\n",
      "Processed 12000 images at 22:45:57\n",
      "Processed 12500 images at 22:46:05\n",
      "Processed 13000 images at 22:46:14\n",
      "Processed 13500 images at 22:46:23\n",
      "Processed 14000 images at 22:46:32\n",
      "Processed 14500 images at 22:46:41\n",
      "Processed 15000 images at 22:46:50\n",
      "Processed 15500 images at 22:46:58\n",
      "Processed 16000 images at 22:47:07\n",
      "Processed 16500 images at 22:47:15\n",
      "Processed 17000 images at 22:47:24\n",
      "Processed 17500 images at 22:47:33\n",
      "Processed 18000 images at 22:47:41\n",
      "Processed 18500 images at 22:47:50\n",
      "Processed 19000 images at 22:47:58\n",
      "Processed 19500 images at 22:48:07\n",
      "Processed 20000 images at 22:48:15\n",
      "Processed 20500 images at 22:48:24\n",
      "Processed 21000 images at 22:48:32\n",
      "Processed 21500 images at 22:48:41\n",
      "Processed 22000 images at 22:48:49\n",
      "Processed 22500 images at 22:48:58\n",
      "Processed 23000 images at 22:49:08\n",
      "Processed 23500 images at 22:49:18\n",
      "Processed 24000 images at 22:49:28\n",
      "Processed 24500 images at 22:49:37\n",
      "Processed 25000 images at 22:49:45\n",
      "Processed 25500 images at 22:49:54\n",
      "Processed 26000 images at 22:50:02\n",
      "Processed 26500 images at 22:50:11\n",
      "Processed 27000 images at 22:50:19\n",
      "Processed 27500 images at 22:50:27\n",
      "Processed 28000 images at 22:50:36\n",
      "Processed 28500 images at 22:50:44\n",
      "Processed 29000 images at 22:50:53\n",
      "Processed 29500 images at 22:51:01\n",
      "Processed 30000 images at 22:51:09\n",
      "Processed 30500 images at 22:51:18\n",
      "Processed 31000 images at 22:51:27\n",
      "Processed 31500 images at 22:51:35\n",
      "Processed 32000 images at 22:51:44\n",
      "Processed 32500 images at 22:51:52\n",
      "Processed 33000 images at 22:52:01\n",
      "Processed 33500 images at 22:52:09\n",
      "Processed 34000 images at 22:52:18\n",
      "Processed 34500 images at 22:52:27\n",
      "Processed 35000 images at 22:52:36\n",
      "Processed 35500 images at 22:52:45\n",
      "Processed 36000 images at 22:52:54\n",
      "Processed 36500 images at 22:53:02\n",
      "Processed 37000 images at 22:53:11\n",
      "Processed 37500 images at 22:53:20\n",
      "Processed 38000 images at 22:53:29\n",
      "Processed 38500 images at 22:53:38\n",
      "Processed 39000 images at 22:53:47\n",
      "Processed 39500 images at 22:53:56\n",
      "Processed 40000 images at 22:54:04\n",
      "Processed 40500 images at 22:54:13\n",
      "Processed 41000 images at 22:54:23\n",
      "Processed 41500 images at 22:54:32\n",
      "Processed 42000 images at 22:54:41\n",
      "Processed 42500 images at 22:54:50\n",
      "Processed 43000 images at 22:54:58\n",
      "Processed 43500 images at 22:55:07\n",
      "Processed 44000 images at 22:55:15\n",
      "Processed 44500 images at 22:55:24\n",
      "Processed 45000 images at 22:55:32\n",
      "Processed 45500 images at 22:55:40\n",
      "Processed 46000 images at 22:55:48\n",
      "Processed 46500 images at 22:55:56\n",
      "Processed 47000 images at 22:56:05\n",
      "Processed 47500 images at 22:56:13\n",
      "Processed 48000 images at 22:56:22\n",
      "Processed 48500 images at 22:56:30\n",
      "Processed 49000 images at 22:56:38\n",
      "Processed 49500 images at 22:56:47\n",
      "Processed 50000 images at 22:56:55\n",
      "Processed 50500 images at 22:57:03\n",
      "Processed 51000 images at 22:57:12\n",
      "Processed 51500 images at 22:57:20\n",
      "Processed 52000 images at 22:57:28\n",
      "Processed 52500 images at 22:57:37\n",
      "Processed 53000 images at 22:57:46\n",
      "Processed 53500 images at 22:57:54\n",
      "Processed 54000 images at 22:58:04\n",
      "Processed 54500 images at 22:58:13\n",
      "Processed 55000 images at 22:58:21\n",
      "Processed 55500 images at 22:58:30\n",
      "Processed 56000 images at 22:58:38\n",
      "Processed 56500 images at 22:58:47\n",
      "Processed 57000 images at 22:58:55\n",
      "Processed 57500 images at 22:59:04\n",
      "Processed 58000 images at 22:59:12\n",
      "Processed 58500 images at 22:59:21\n",
      "Processed 59000 images at 22:59:30\n",
      "Processed 59500 images at 22:59:38\n",
      "Processed 60000 images at 22:59:47\n",
      "Processed 60500 images at 22:59:55\n",
      "Processed 61000 images at 23:00:04\n",
      "Processed 61500 images at 23:00:13\n",
      "Processed 62000 images at 23:00:22\n",
      "Processed 62500 images at 23:00:32\n",
      "Processed 63000 images at 23:00:41\n",
      "Processed 63500 images at 23:00:50\n",
      "Processed 64000 images at 23:00:58\n",
      "Processed 64500 images at 23:01:06\n",
      "Processed 65000 images at 23:01:15\n",
      "Processed 65500 images at 23:01:24\n",
      "Processed 66000 images at 23:01:33\n",
      "Processed 66500 images at 23:01:41\n",
      "Processed 67000 images at 23:01:50\n",
      "Processed 67500 images at 23:01:58\n",
      "Processed 68000 images at 23:02:07\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for index, filename in enumerate(filenamesList):\n",
    "    filepath = os.path.join(imageDir, filename)\n",
    "    # STEP 3: Load the input image.\n",
    "    image = mp.Image.create_from_file(filepath)\n",
    "\n",
    "    # STEP 4: Detect face landmarks from the input image.\n",
    "    detection_result = detector.detect(image)\n",
    "\n",
    "    #print(len(detection_result.face_landmarks))\n",
    "    if len(detection_result.face_landmarks) == 1:\n",
    "        xs = []\n",
    "        ys = []\n",
    "        zs = []\n",
    "        for face in detection_result.face_landmarks:\n",
    "            rowIndex = data[data['image_name'] == filename].index[0]\n",
    "            expr = data.at[rowIndex, 'expression_label']\n",
    "            for id, landmark in enumerate(face):\n",
    "                xPure = landmark.x\n",
    "                yPure = landmark.y\n",
    "                zPure = landmark.z\n",
    "                \n",
    "\n",
    "                xs.append(xPure)\n",
    "                ys.append(yPure)\n",
    "                zs.append(zPure)\n",
    "        xJSON = json.dumps(xs)\n",
    "        yJSON = json.dumps(ys)\n",
    "        zJSON = json.dumps(zs)\n",
    "\n",
    "        # Insert the data into the table\n",
    "        cursor.execute('''\n",
    "        INSERT INTO landmarksInput (NAME, EXPR, X, Y, Z)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (filename, expr, xJSON, yJSON, zJSON))\n",
    "\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "        xs, ys = affineTransform(xs, ys)\n",
    "        xs, ys, zs = scalePointsFixed(xs, ys, zs)\n",
    "        xs, ys = alignNormalizedPointsFixed(xs, ys)\n",
    "    \n",
    "        xJSON = json.dumps(xs)\n",
    "        yJSON = json.dumps(ys)\n",
    "        zJSON = json.dumps(zs)\n",
    "        # Insert the data into the table\n",
    "        cursor.execute('''\n",
    "        INSERT INTO landmarksTransform (NAME, EXPR, X, Y, Z)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "        ''', (filename, expr, xJSON, yJSON, zJSON))\n",
    "        # Commit the changes\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "    if (index+1) % 500 == 0:\n",
    "\n",
    "        # Get the current date and time\n",
    "        now = datetime.datetime.now()\n",
    "\n",
    "        # Format the current time\n",
    "        formatted_time = now.strftime(\"%H:%M:%S\")\n",
    "\n",
    "        print(f'Processed {index+1} images at {formatted_time}')\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "exprs = list(expressionDic.values())\n",
    "\n",
    "dbPath = r'C:\\Users\\myfir\\My Drive\\Skyline\\ExpW\\expw.db'\n",
    "conn = sqlite3.connect(dbPath)\n",
    "\n",
    "query = 'SELECT * FROM landmarksTransform WHERE EXPR = ?'\n",
    "\n",
    "for expr in exprs:\n",
    "    cursor = conn.cursor()\n",
    "    X = np.array()\n",
    "    Y = np.array()\n",
    "    Z = np.array()\n",
    "\n",
    "    coords = [X, Y, Z]\n",
    "    # Execute a query to retrieve all rows from the table\n",
    "    cursor.execute(query, (expr,))\n",
    "    rows = cursor.fetchall()\n",
    "    for row in rows:\n",
    "        X = np.concatenate((X, np.array(json.loads(row['X']))))\n",
    "        Y = np.concatenate((Y, np.array(json.loads(row['Y']))))\n",
    "        Z = np.concatenate((Z, np.array(json.loads(row['Z']))))\n",
    "\n",
    "    XSample = X.reshape((-1, 478))\n",
    "    YSample = Y.reshape((-1, 478))\n",
    "    ZSample = Z.reshape((-1, 478))\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fetch all results and load into a pandas DataFrame\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "rows = cursor.fetchall()\n",
    "data = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n",
    "import ast\n",
    "\n",
    "# Convert the 'X', 'Y', and 'Z' columns from strings to lists\n",
    "data['X'] = data['X'].apply(ast.literal_eval)\n",
    "data['Y'] = data['Y'].apply(ast.literal_eval)\n",
    "data['Z'] = data['Z'].apply(ast.literal_eval)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
